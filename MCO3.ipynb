{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code below are retrieved from a Python library, sklearn.  Code was modified in order to remove unrelated code segments, and add frequency counters.\n",
    "\n",
    "#### The original code was retrieved from https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext Cython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "'''_k_means_elkan.pyx'''\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "from cython cimport floating\n",
    "\n",
    "from libc.math cimport sqrt\n",
    "\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.cluster._k_means import _centers_dense\n",
    "from sklearn.utils.fixes import partition\n",
    "\n",
    "\n",
    "cdef floating euclidian_dist(floating* a, floating* b, int n_features) nogil:\n",
    "    cdef floating result, tmp\n",
    "    result = 0\n",
    "    cdef int i\n",
    "    for i in range(n_features):\n",
    "        tmp = (a[i] - b[i])\n",
    "        result += tmp * tmp\n",
    "    return sqrt(result)\n",
    "\n",
    "\n",
    "cdef update_labels_distances_inplace(\n",
    "        floating* X, floating* centers, floating[:, :] center_half_distances,\n",
    "        int[:] labels, floating[:, :] lower_bounds, floating[:] upper_bounds,\n",
    "        int n_samples, int n_features, int n_clusters):\n",
    "    # assigns closest center to X\n",
    "    # uses triangle inequality\n",
    "    cdef floating* x\n",
    "    cdef floating* c\n",
    "    cdef floating d_c, dist\n",
    "    cdef int c_x, j, sample\n",
    "    for sample in range(n_samples):\n",
    "        # assign first cluster center\n",
    "        c_x = 0\n",
    "        x = X + sample * n_features\n",
    "        d_c = euclidian_dist(x, centers, n_features)\n",
    "        lower_bounds[sample, 0] = d_c\n",
    "        for j in range(1, n_clusters):\n",
    "            if d_c > center_half_distances[c_x, j]:\n",
    "                c = centers + j * n_features\n",
    "                dist = euclidian_dist(x, c, n_features)\n",
    "                lower_bounds[sample, j] = dist\n",
    "                if dist < d_c:\n",
    "                    d_c = dist\n",
    "                    c_x = j\n",
    "        labels[sample] = c_x\n",
    "        upper_bounds[sample] = d_c\n",
    "\n",
    "\n",
    "def k_means_elkan(np.ndarray[floating, ndim=2, mode='c'] X_, int n_clusters,\n",
    "                  np.ndarray[floating, ndim=2, mode='c'] init,\n",
    "                  float tol=1e-4, int max_iter=30, verbose=False):\n",
    "    if floating is float:\n",
    "        dtype = np.float32\n",
    "    else:\n",
    "        dtype = np.float64\n",
    "\n",
    "   #initialize\n",
    "    cdef np.ndarray[floating, ndim=2, mode='c'] centers_ = init\n",
    "    cdef floating* centers_p = <floating*>centers_.data\n",
    "    cdef floating* X_p = <floating*>X_.data\n",
    "    cdef floating* x_p\n",
    "    cdef Py_ssize_t n_samples = X_.shape[0]\n",
    "    cdef Py_ssize_t n_features = X_.shape[1]\n",
    "    cdef int point_index, center_index, label\n",
    "    cdef floating upper_bound, distance\n",
    "    cdef floating[:, :] center_half_distances = euclidean_distances(centers_) / 2.\n",
    "    cdef floating[:, :] lower_bounds = np.zeros((n_samples, n_clusters), dtype=dtype)\n",
    "    cdef floating[:] distance_next_center\n",
    "    labels_ = np.empty(n_samples, dtype=np.int32)\n",
    "    cdef int[:] labels = labels_\n",
    "    upper_bounds_ = np.empty(n_samples, dtype=dtype)\n",
    "    cdef floating[:] upper_bounds = upper_bounds_\n",
    "\n",
    "    # Get the inital set of upper bounds and lower bounds for each sample.\n",
    "    update_labels_distances_inplace(X_p, centers_p, center_half_distances,\n",
    "                                    labels, lower_bounds, upper_bounds,\n",
    "                                    n_samples, n_features, n_clusters)\n",
    "    cdef np.uint8_t[:] bounds_tight = np.ones(n_samples, dtype=np.uint8)\n",
    "    cdef np.uint8_t[:] points_to_update = np.zeros(n_samples, dtype=np.uint8)\n",
    "    cdef np.ndarray[floating, ndim=2, mode='c'] new_centers\n",
    "\n",
    "    if max_iter <= 0:\n",
    "        raise ValueError('Number of iterations should be a positive number'\n",
    "        ', got %d instead' % max_iter)\n",
    "\n",
    "    col_indices = np.arange(center_half_distances.shape[0], dtype=np.int)\n",
    "    for iteration in range(max_iter):\n",
    "        if verbose:\n",
    "            print(\"start iteration\")\n",
    "\n",
    "        cd =  np.asarray(center_half_distances)\n",
    "        distance_next_center = partition(cd, kth=1, axis=0)[1]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"done sorting\")\n",
    "\n",
    "        for point_index in range(n_samples):\n",
    "            upper_bound = upper_bounds[point_index]\n",
    "            label = labels[point_index]\n",
    "\n",
    "            # This means that the next likely center is far away from the\n",
    "            # currently assigned center and the sample is unlikely to be\n",
    "            # reassigned.\n",
    "            if distance_next_center[label] >= upper_bound:\n",
    "                continue\n",
    "            x_p = X_p + point_index * n_features\n",
    "\n",
    "            # TODO: get pointer to lower_bounds[point_index, center_index]\n",
    "            for center_index in range(n_clusters):\n",
    "\n",
    "                # If this holds, then center_index is a good candidate for the\n",
    "                # sample to be relabelled, and we need to confirm this by\n",
    "                # recomputing the upper and lower bounds.\n",
    "                if (center_index != label\n",
    "                        and (upper_bound > lower_bounds[point_index, center_index])\n",
    "                        and (upper_bound > center_half_distances[center_index, label])):\n",
    "\n",
    "                    # Recompute the upper bound by calculating the actual distance\n",
    "                    # between the sample and label.\n",
    "                    if not bounds_tight[point_index]:\n",
    "                        upper_bound = euclidian_dist(x_p, centers_p + label * n_features, n_features)\n",
    "                        lower_bounds[point_index, label] = upper_bound\n",
    "                        bounds_tight[point_index] = 1\n",
    "\n",
    "                    # If the condition still holds, then compute the actual distance between\n",
    "                    # the sample and center_index. If this is still lesser than the previous\n",
    "                    # distance, reassign labels.\n",
    "                    if (upper_bound > lower_bounds[point_index, center_index]\n",
    "                            or (upper_bound > center_half_distances[label, center_index])):\n",
    "                        distance = euclidian_dist(x_p, centers_p + center_index * n_features, n_features)\n",
    "                        lower_bounds[point_index, center_index] = distance\n",
    "                        if distance < upper_bound:\n",
    "                            label = center_index\n",
    "                            upper_bound = distance\n",
    "\n",
    "            labels[point_index] = label\n",
    "            upper_bounds[point_index] = upper_bound\n",
    "\n",
    "        if verbose:\n",
    "            print(\"end inner loop\")\n",
    "\n",
    "        # compute new centers\n",
    "        new_centers = _centers_dense(X_, labels_, n_clusters, upper_bounds_)\n",
    "        bounds_tight[:] = 0\n",
    "\n",
    "        # compute distance each center moved\n",
    "        center_shift = np.sqrt(np.sum((centers_ - new_centers) ** 2, axis=1))\n",
    "\n",
    "        # update bounds accordingly\n",
    "        lower_bounds = np.maximum(lower_bounds - center_shift, 0)\n",
    "        upper_bounds = upper_bounds + center_shift[labels_]\n",
    "\n",
    "        # reassign centers\n",
    "        centers_ = new_centers\n",
    "        centers_p = <floating*>new_centers.data\n",
    "\n",
    "        # update between-center distances\n",
    "        center_half_distances = euclidean_distances(centers_) / 2.\n",
    "        if verbose:\n",
    "            print('Iteration %i, inertia %s'\n",
    "                  % (iteration, np.sum((X_ - centers_[labels]) ** 2)))\n",
    "        center_shift_total = np.sum(center_shift)\n",
    "        if center_shift_total ** 2 < tol:\n",
    "            if verbose:\n",
    "                print(\"center shift %e within tolerance %e\"\n",
    "                      % (center_shift_total, tol))\n",
    "            break\n",
    "\n",
    "    # We need this to make sure that the labels give the same output as\n",
    "    # predict(X)\n",
    "    if center_shift_total > 0:\n",
    "        update_labels_distances_inplace(X_p, centers_p, center_half_distances,\n",
    "                                        labels, lower_bounds, upper_bounds,\n",
    "                                        n_samples, n_features, n_clusters)\n",
    "    return centers_, labels_, iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''k_means_.py'''\n",
    "\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClusterMixin, TransformerMixin\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin_min\n",
    "from sklearn.utils.extmath import row_norms, squared_norm, stable_cumsum\n",
    "from sklearn.utils.sparsefuncs_fast import assign_rows_csr\n",
    "from sklearn.utils.sparsefuncs import mean_variance_axis\n",
    "from sklearn.utils.fixes import astype\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import as_float_array\n",
    "from sklearn.utils import gen_batches\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils.validation import FLOAT_DTYPES\n",
    "from sklearn.utils.random import choice\n",
    "from sklearn.externals.joblib import Parallel\n",
    "from sklearn.externals.joblib import delayed\n",
    "from sklearn.externals.six import string_types\n",
    "\n",
    "class KMeans(BaseEstimator, ClusterMixin, TransformerMixin):\n",
    "    def __init__(self, n_clusters=8, init='k-means++', n_init=10,\n",
    "                 max_iter=300, tol=1e-4, precompute_distances='auto',\n",
    "                 verbose=0, random_state=None, copy_x=True,\n",
    "                 n_jobs=1, algorithm='auto'):\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.init = init\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.precompute_distances = precompute_distances\n",
    "        self.n_init = n_init\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state #sets the random seed. With the seed reset (every time), the same set of numbers will appear every time.\n",
    "\n",
    "\n",
    "        self.copy_x = copy_x\n",
    "        self.n_jobs = n_jobs\n",
    "        self.algorithm = algorithm\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        X = self._check_fit_data(X) #verifying that the sample is valid.\n",
    "\n",
    "        self.cluster_centers_, self.labels_, self.inertia_, self.n_iter_ = \\\n",
    "            k_means(\n",
    "                X, n_clusters=self.n_clusters, init=self.init,\n",
    "                n_init=self.n_init, max_iter=self.max_iter, verbose=self.verbose,\n",
    "                precompute_distances=self.precompute_distances,\n",
    "                tol=self.tol, random_state=random_state, copy_x=self.copy_x,\n",
    "                n_jobs=self.n_jobs, algorithm=self.algorithm,\n",
    "                return_n_iter=True)\n",
    "        return self\n",
    "    \n",
    "    def _check_fit_data(self, X):\n",
    "        \"\"\"Verify that the number of samples given is larger than k\"\"\"\n",
    "        X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32])\n",
    "        if X.shape[0] < self.n_clusters:\n",
    "            raise ValueError(\"n_samples=%d should be >= n_clusters=%d\" % (\n",
    "                X.shape[0], self.n_clusters))\n",
    "        return X\n",
    "\n",
    "def k_means(X, n_clusters, init='k-means++', precompute_distances='auto',\n",
    "            n_init=10, max_iter=300, verbose=False,\n",
    "            tol=1e-4, random_state=None, copy_x=True, n_jobs=1,\n",
    "            algorithm=\"auto\", return_n_iter=False):\n",
    "\n",
    "    if n_init <= 0:\n",
    "        raise ValueError(\"Invalid number of initializations.\"\n",
    "                         \" n_init=%d must be bigger than zero.\" % n_init)\n",
    "    if max_iter <= 0:\n",
    "        raise ValueError('Number of iterations should be a positive number,'\n",
    "                         ' got %d instead' % max_iter)\n",
    "\n",
    "    X = as_float_array(X, copy=copy_x)\n",
    "    tol = _tolerance(X, tol)\n",
    "\n",
    "    # If the distances are precomputed every job will create a matrix of shape\n",
    "    # (n_clusters, n_samples). To stop KMeans from eating up memory we only\n",
    "    # activate this if the created matrix is guaranteed to be under 100MB. 12\n",
    "    # million entries consume a little under 100MB if they are of type double.\n",
    "    if precompute_distances == 'auto':\n",
    "        n_samples = X.shape[0]\n",
    "        precompute_distances = (n_clusters * n_samples) < 12e6\n",
    "    elif isinstance(precompute_distances, bool):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"precompute_distances should be 'auto' or True/False\"\n",
    "                         \", but a value of %r was passed\" %\n",
    "                         precompute_distances)\n",
    "\n",
    "    # subtract of mean of x for more accurate distance computations\n",
    "    if not sp.issparse(X):\n",
    "        X_mean = X.mean(axis=0)\n",
    "        # The copy was already done above\n",
    "        X -= X_mean\n",
    "\n",
    "        if hasattr(init, '__array__'):\n",
    "            init -= X_mean\n",
    "\n",
    "    # precompute squared norms of data points\n",
    "    x_squared_norms = row_norms(X, squared=True)\n",
    "\n",
    "    best_labels, best_inertia, best_centers = None, None, None\n",
    "    if n_clusters == 1:\n",
    "        # elkan doesn't make sense for a single cluster, full will produce\n",
    "        # the right result.\n",
    "        algorithm = \"full\"\n",
    "    if algorithm == \"auto\":\n",
    "        algorithm = \"full\" if sp.issparse(X) else 'elkan'\n",
    "    if algorithm == \"full\":\n",
    "        kmeans_single = _kmeans_single_lloyd\n",
    "    elif algorithm == \"elkan\":\n",
    "        kmeans_single = _kmeans_single_elkan\n",
    "    else:\n",
    "        raise ValueError(\"Algorithm must be 'auto', 'full' or 'elkan', got\"\n",
    "                         \" %s\" % str(algorithm))\n",
    "\n",
    "    \n",
    "    \n",
    "    for it in range(n_init):\n",
    "        # run a k-means once\n",
    "        labels, inertia, centers, n_iter_ = kmeans_single(\n",
    "            X, n_clusters, max_iter=max_iter, init=init, verbose=verbose,\n",
    "            precompute_distances=precompute_distances, tol=tol,\n",
    "            x_squared_norms=x_squared_norms, random_state=random_state)\n",
    "        # determine if these results are the best so far\n",
    "        if best_inertia is None or inertia < best_inertia:\n",
    "            best_labels = labels.copy()\n",
    "            best_centers = centers.copy()\n",
    "            best_inertia = inertia\n",
    "            best_n_iter = n_iter_\n",
    "\n",
    "    if not sp.issparse(X):\n",
    "        if not copy_x:\n",
    "            X += X_mean\n",
    "        best_centers += X_mean\n",
    "\n",
    "    if return_n_iter:\n",
    "        return best_centers, best_labels, best_inertia, best_n_iter\n",
    "    else:\n",
    "        return best_centers, best_labels, best_inertia\n",
    "\n",
    "def _tolerance(X, tol):\n",
    "    \"\"\"Return a tolerance which is independent of the dataset\"\"\"\n",
    "    if sp.issparse(X): \n",
    "        variances = mean_variance_axis(X, axis=0)[1]\n",
    "    else:\n",
    "        variances = np.var(X, axis=0)\n",
    "    return np.mean(variances) * tol\n",
    "\n",
    "def _kmeans_single_elkan(X, n_clusters, max_iter=300, init='k-means++',\n",
    "                         verbose=False, x_squared_norms=None,\n",
    "                         random_state=None, tol=1e-4,\n",
    "                         precompute_distances=True):\n",
    "    if sp.issparse(X):\n",
    "        raise ValueError(\"algorithm='elkan' not supported for sparse input X\")\n",
    "    X = check_array(X, order=\"C\")\n",
    "    if x_squared_norms is None:\n",
    "        x_squared_norms = row_norms(X, squared=True)\n",
    "    # init\n",
    "    centers = _init_centroids(X, n_clusters, init, random_state=random_state,\n",
    "                              x_squared_norms=x_squared_norms)\n",
    "    centers = np.ascontiguousarray(centers)\n",
    "    if verbose:\n",
    "        print('Initialization complete')\n",
    "    centers, labels, n_iter = k_means_elkan(X, n_clusters, centers, tol=tol,\n",
    "                                            max_iter=max_iter, verbose=verbose)\n",
    "    inertia = np.sum((X - centers[labels]) ** 2, dtype=np.float64)\n",
    "    return labels, inertia, centers, n_iter\n",
    "\n",
    "def _init_centroids(X, k, init, random_state=None, x_squared_norms=None,\n",
    "                    init_size=None):\n",
    "    random_state = check_random_state(random_state)\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    if x_squared_norms is None:\n",
    "        x_squared_norms = row_norms(X, squared=True)\n",
    "\n",
    "    if init_size is not None and init_size < n_samples:\n",
    "        if init_size < k:\n",
    "            warnings.warn(\n",
    "                \"init_size=%d should be larger than k=%d. \"\n",
    "                \"Setting it to 3*k\" % (init_size, k),\n",
    "                RuntimeWarning, stacklevel=2)\n",
    "            init_size = 3 * k\n",
    "        init_indices = random_state.randint(0, n_samples, init_size)\n",
    "        X = X[init_indices]\n",
    "        x_squared_norms = x_squared_norms[init_indices]\n",
    "        n_samples = X.shape[0]\n",
    "    elif n_samples < k:\n",
    "        raise ValueError(\n",
    "            \"n_samples=%d should be larger than k=%d\" % (n_samples, k))\n",
    "\n",
    "    if isinstance(init, string_types) and init == 'k-means++':\n",
    "        centers = _k_init(X, k, random_state=random_state,\n",
    "                          x_squared_norms=x_squared_norms)\n",
    "    elif isinstance(init, string_types) and init == 'random':\n",
    "        seeds = random_state.permutation(n_samples)[:k]\n",
    "        centers = X[seeds]\n",
    "    elif hasattr(init, '__array__'):\n",
    "        # ensure that the centers have the same dtype as X\n",
    "        # this is a requirement of fused types of cython\n",
    "        centers = np.array(init, dtype=X.dtype)\n",
    "    elif callable(init):\n",
    "        centers = init(X, k, random_state=random_state)\n",
    "        centers = np.asarray(centers, dtype=X.dtype)\n",
    "    else:\n",
    "        raise ValueError(\"the init parameter for the k-means should \"\n",
    "                         \"be 'k-means++' or 'random' or an ndarray, \"\n",
    "                         \"'%s' (type '%s') was passed.\" % (init, type(init)))\n",
    "\n",
    "    if sp.issparse(centers):\n",
    "        centers = centers.toarray()\n",
    "\n",
    "    return centers\n",
    "\n",
    "def _k_init(X, n_clusters, x_squared_norms, random_state, n_local_trials=None):\n",
    "    \"\"\"Init n_clusters seeds according to k-means++\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    centers = np.empty((n_clusters, n_features), dtype=X.dtype)\n",
    "\n",
    "    assert x_squared_norms is not None, 'x_squared_norms None in _k_init'\n",
    "\n",
    "    # Set the number of local seeding trials if none is given\n",
    "    if n_local_trials is None:\n",
    "        # This is what Arthur/Vassilvitskii tried, but did not report\n",
    "        # specific results for other than mentioning in the conclusion\n",
    "        # that it helped.\n",
    "        n_local_trials = 2 + int(np.log(n_clusters))\n",
    "\n",
    "    # Pick first center randomly\n",
    "    center_id = random_state.randint(n_samples)\n",
    "    if sp.issparse(X):\n",
    "        centers[0] = X[center_id].toarray()\n",
    "    else:\n",
    "        centers[0] = X[center_id]\n",
    "\n",
    "    # Initialize list of closest distances and calculate current potential\n",
    "    closest_dist_sq = euclidean_distances(\n",
    "        centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms,\n",
    "        squared=True)\n",
    "    current_pot = closest_dist_sq.sum()\n",
    "\n",
    "    # Pick the remaining n_clusters-1 points\n",
    "    for c in range(1, n_clusters):\n",
    "        # Choose center candidates by sampling with probability proportional\n",
    "        # to the squared distance to the closest existing center\n",
    "        rand_vals = random_state.random_sample(n_local_trials) * current_pot\n",
    "        candidate_ids = np.searchsorted(stable_cumsum(closest_dist_sq),\n",
    "                                        rand_vals)\n",
    "\n",
    "        # Compute distances to center candidates\n",
    "        distance_to_candidates = euclidean_distances(\n",
    "            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)\n",
    "\n",
    "        # Decide which candidate is the best\n",
    "        best_candidate = None\n",
    "        best_pot = None\n",
    "        best_dist_sq = None\n",
    "        for trial in range(n_local_trials):\n",
    "            # Compute potential when including center candidate\n",
    "            new_dist_sq = np.minimum(closest_dist_sq,\n",
    "                                     distance_to_candidates[trial])\n",
    "            new_pot = new_dist_sq.sum()\n",
    "\n",
    "            # Store result if it is the best local trial so far\n",
    "            if (best_candidate is None) or (new_pot < best_pot):\n",
    "                best_candidate = candidate_ids[trial]\n",
    "                best_pot = new_pot\n",
    "                best_dist_sq = new_dist_sq\n",
    "\n",
    "        # Permanently add best center candidate found in local tries\n",
    "        if sp.issparse(X):\n",
    "            centers[c] = X[best_candidate].toarray()\n",
    "        else:\n",
    "            centers[c] = X[best_candidate]\n",
    "        current_pot = best_pot\n",
    "        closest_dist_sq = best_dist_sq\n",
    "\n",
    "    return centers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
